{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f7ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parent_dir = os.path.abspath(os.getcwd())\n",
    "\n",
    "# 遍历所有子目录并将它们添加到sys.path中\n",
    "for dirpath, dirnames, filenames in os.walk(parent_dir):\n",
    "    for dirname in dirnames:\n",
    "        sys.path.append(os.path.abspath(os.path.join(dirpath, dirname)))\n",
    "\n",
    "\n",
    "import time\n",
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "from ipywidgets import Output, GridspecLayout\n",
    "from IPython.display import display, Audio, Javascript, clear_output\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from voice_conversion.Utils.JDC.model import JDCNet\n",
    "from voice_conversion.Models.models import Generator, MappingNetwork, StyleEncoder\n",
    "from munch import Munch\n",
    "from parallel_wavegan.utils import load_model\n",
    "import torch\n",
    "import torchaudio\n",
    "import yaml\n",
    "import noisereduce as nr\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312b8858",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31326224",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "sr = 24000\n",
    "\n",
    "F = 'female'\n",
    "M = 'male'\n",
    "device = 'cpu'\n",
    "\n",
    "#speaker related\n",
    "speaker_file = \"dataset/Data/train_list.txt\"\n",
    "speakers_num = 20\n",
    "pseudo_names = {11: 'Joe', 12: 'Mark', 13: 'Tony', 14: 'Tom', 15: 'Anna', 16: 'Jane', 17: 'Mary', 18: 'Julia', 19: 'Lizy', 20: 'Harry'}\n",
    "gender_map = {11: M, 12: M, 13: M, 14: M, 15: F, 16: F, 17: F, 18: F, 19: F, 20: M}\n",
    "selected_speakers = [15, 13, 16, 14, 17, 19]\n",
    "\n",
    "# models' paths\n",
    "f0_path = 'voice_conversion/Utils/JDC/bst.t7'\n",
    "vocoder_path = 'checkpoints/checkpoint-2500000steps.pkl'\n",
    "stargan_dir = '/scratch/sghosh/Hannover_Demo/v2/utility_components/starganv2/'\n",
    "stargan_model_name = 'stargan_emo.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b37352",
   "metadata": {},
   "source": [
    "## Utility Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf19c26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spkr_name(fname):\n",
    "    fnm_arr = file_name.split('/')\n",
    "    if 'ESD' in fname:\n",
    "        name = fnm_arr[-1]\n",
    "        name = name.split('_')\n",
    "        name = name[0].replace('00', '')\n",
    "    else:\n",
    "        name = fnm_arr[-2]\n",
    "    return name\n",
    "\n",
    "\n",
    "# Get a speaker dict from config\n",
    "speakers_20 = np.zeros((speakers_num), dtype=int)\n",
    "with open(speaker_file) as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "speaker_dicts_20 = {}\n",
    "\n",
    "for line in lines:\n",
    "    file_name, sp_id = line.split('|')\n",
    "    sp_id = int(sp_id)\n",
    "    spkr_nm = get_spkr_name(file_name)\n",
    "    speaker_dicts_20[spkr_nm] = sp_id\n",
    "    speakers_20[sp_id] = int(spkr_nm.replace('p', ''))\n",
    "speakers_20 = speakers_20.tolist()\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300)\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "class DefaultDict(dict):\n",
    "    \"\"\"Implementation of perl's autovivification feature.\"\"\"\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        try:\n",
    "            return dict.__getitem__(self, item)\n",
    "        except KeyError:\n",
    "            value = self[item] = type(self)()\n",
    "            return value\n",
    "\n",
    "\n",
    "def compute_style(starganv2, slct_spkrs, speaker_dicts):\n",
    "    '''get speaker embeddings'''\n",
    "    reference_embeddings = DefaultDict()\n",
    "    for sp in slct_spkrs:\n",
    "        key = str(sp)\n",
    "        speaker = int(speaker_dicts[key])\n",
    "        label = torch.LongTensor([speaker]).to(device)\n",
    "        latent_dim = starganv2.mapping_network.shared[0].in_features\n",
    "        ref = starganv2.mapping_network(\n",
    "            torch.randn(1, latent_dim).to(device), label)\n",
    "        reference_embeddings[key] = (ref, label)\n",
    "\n",
    "    return reference_embeddings\n",
    "\n",
    "\n",
    "def build_inference_pipeline(device='cuda'):\n",
    "\n",
    "    def build_model(model_params):\n",
    "        args = Munch(model_params)\n",
    "        generator = Generator(args.dim_in, args.style_dim, args.max_conv_dim,\n",
    "                              w_hpf=args.w_hpf, F0_channel=args.F0_channel)\n",
    "\n",
    "        # Can use both to generate embeddings...\n",
    "        mapping_network = MappingNetwork(\n",
    "            args.latent_dim, args.style_dim, args.num_domains, hidden_dim=args.max_conv_dim)\n",
    "        style_encoder = StyleEncoder(\n",
    "            args.dim_in, args.style_dim, args.num_domains, args.max_conv_dim)\n",
    "\n",
    "        nets_ema = Munch(generator=generator,\n",
    "                         mapping_network=mapping_network,\n",
    "                         style_encoder=style_encoder)\n",
    "\n",
    "        return nets_ema\n",
    "\n",
    "    # load F0 model\n",
    "    F0_model = JDCNet(num_class=1, seq_len=192)\n",
    "    params = torch.load(f0_path, map_location=device)['net']\n",
    "    F0_model.load_state_dict(params)\n",
    "    _ = F0_model.eval()\n",
    "    F0_model = F0_model.to(device)\n",
    "\n",
    "    # load hifigan vocoder\n",
    "    vocoder = load_model(vocoder_path).to(device).eval()\n",
    "    vocoder.remove_weight_norm()\n",
    "    _ = vocoder.eval()\n",
    "\n",
    "    # load emo-stargan model\n",
    "    with open(os.path.join(stargan_dir, 'config.yml')) as f:\n",
    "        emostargan_config = yaml.safe_load(f)\n",
    "\n",
    "    emostargan = build_model(model_params=emostargan_config[\"model_params\"])\n",
    "    params = torch.load(os.path.join(\n",
    "        stargan_dir, stargan_model_name), map_location=device)\n",
    "    params = params['model_ema']\n",
    "    _ = [emostargan[key].load_state_dict(\n",
    "        params[key], strict=False) for key in emostargan]\n",
    "    _ = [emostargan[key].eval() for key in emostargan]\n",
    "    emostargan.style_encoder = emostargan.style_encoder.to(device)\n",
    "    emostargan.mapping_network = emostargan.mapping_network.to(device)\n",
    "    emostargan.generator = emostargan.generator.to(device)\n",
    "\n",
    "    # initialise speaker details from config file\n",
    "    reference_embeddings = compute_style(\n",
    "        emostargan, selected_speakers, speaker_dicts_20)\n",
    "\n",
    "    return emostargan, F0_model, vocoder, reference_embeddings\n",
    "\n",
    "\n",
    "# Little utils\n",
    "def get_F0(wav):\n",
    "    f0, _, _ = librosa.pyin(wav, fmin=librosa.note_to_hz(\n",
    "        'C2'), fmax=librosa.note_to_hz('C7'))\n",
    "    return f0\n",
    "\n",
    "\n",
    "def get_melspec(wav):\n",
    "    return librosa.amplitude_to_db(np.abs(librosa.stft(wav)), ref=np.max)\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def generate_conversions(wav, starganv2, reference_embeddings, F0_model, vocoder, noise_reduce=False):\n",
    "    '''\n",
    "    generate conversion for the selected speakers, whose speaker embeddings were generated\n",
    "    Args:\n",
    "        wav:\n",
    "        starganv2:\n",
    "        reference_embeddings:\n",
    "        F0_model:\n",
    "        vocoder:\n",
    "        noise_reduce:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    source = preprocess(wav)\n",
    "    t = []\n",
    "    keys = []\n",
    "    reconstructed_samples = DefaultDict()\n",
    "   # print('Target Speakers:')\n",
    "    for key, (ref, _) in reference_embeddings.items():\n",
    "        print(pseudo_names[int(key.replace('p', ''))])\n",
    "        # start = time.time()\n",
    "        with torch.no_grad():\n",
    "            f0_feat = F0_model.get_feature_GAN(source.unsqueeze(1))\n",
    "            out = starganv2.generator(source.unsqueeze(1), ref, F0=f0_feat)\n",
    "            c = out.transpose(-1, -2).squeeze()\n",
    "            if key not in speaker_dicts_20 or speaker_dicts_20[key] == \"\":\n",
    "                recon = None\n",
    "                print(key, 'not in speaker_dicts')\n",
    "            else:\n",
    "                recon = vocoder.inference(c)\n",
    "                recon = recon.view(-1).cpu().numpy()\n",
    "                recon = recon / np.max(np.abs(recon))\n",
    "        # end = time.time()\n",
    "        # t.append(end - start)\n",
    "\n",
    "        reconstructed_samples[key] = recon\n",
    "        keys.append(key)\n",
    "        if noise_reduce:\n",
    "            recon = nr.reduce_noise(y=recon, sr=sr, prop_decrease=0.5)\n",
    "        display(Audio(recon, rate=sr))\n",
    "\n",
    "    # print('Average processing time per speaker: %.3f sec' % (sum(t) / len(t)))\n",
    "    return keys, reconstructed_samples\n",
    "\n",
    "\n",
    "def self_convert(wav, vocoder):\n",
    "    '''convert an audio to mel-spectrogam, and then fed to vocoder to generate the audio. This is needed, so that\n",
    "    its possible to plot the original recording with the converted ones, as theres always some mismatch between original\n",
    "    and vocoder generated samples.'''\n",
    "    mel = preprocess(wav)\n",
    "    c = mel.transpose(-1, -2).squeeze()\n",
    "    with torch.no_grad():\n",
    "        recon_hifi = vocoder.inference(c)\n",
    "        recon_hifi = recon_hifi.view(-1).cpu().numpy()\n",
    "    return recon_hifi\n",
    "\n",
    "\n",
    "def plot_melspec(keys, audios, sample_rate=24000, noise_reduce=False):\n",
    "    ''' displays the converted mel-spectrograms'''\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10), sharex=True, sharey=True)\n",
    "    plt.subplots_adjust(bottom=0.1, hspace=0.3)\n",
    "    idx = 0\n",
    "    idy = 0\n",
    "    f0s = []\n",
    "    for key in keys:\n",
    "        audio_ = audios[key]\n",
    "        gender = gender_map[int(key)]\n",
    "        if noise_reduce:\n",
    "            audio_ = nr.reduce_noise(\n",
    "                y=audio_, sr=sample_rate, prop_decrease=0.5)\n",
    "        librosa.display.specshow(get_melspec(audio_),\n",
    "                                 x_axis='time',\n",
    "                                 y_axis='log',\n",
    "                                 edgecolors='None',\n",
    "                                 ax=axs[idx, idy])\n",
    "        f0 = get_F0(audio_)\n",
    "        f0s.append(f0)\n",
    "        times = librosa.times_like(f0)\n",
    "        axs[idx, idy].plot(times, f0, label='f0', color='cyan', linewidth=2)\n",
    "        axs[idx, idy].set_title(pseudo_names[int(key)],\n",
    "                                color='lightcoral' if gender == F else 'green')\n",
    "\n",
    "        if idy == 2:\n",
    "            idx += 1\n",
    "            idy = 0\n",
    "        else:\n",
    "            idy += 1\n",
    "    plt.show()\n",
    "    return f0s\n",
    "\n",
    "\n",
    "def get_gender_legend():\n",
    "    ''' generate the gender specific legends '''\n",
    "    f_l = mlines.Line2D([], [], color='lightcoral', marker='s', ls='', label=F)\n",
    "    m_l = mlines.Line2D([], [], color='green', marker='s', ls='', label=M)\n",
    "    plt.legend(handles=[f_l, m_l])\n",
    "\n",
    "\n",
    "def get_intensity(audio_):\n",
    "    return librosa.feature.rms(y=audio_, frame_length=150, hop_length=300, center=True)[0]\n",
    "\n",
    "\n",
    "def get_trimmed(self_measure, conv_measure):\n",
    "    if conv_measure.shape > self_measure.shape:\n",
    "        diff = conv_measure.shape[0] - self_measure.shape[0]\n",
    "        conv_measure = conv_measure[:-diff]\n",
    "    elif conv_measure.shape < self_measure.shape:\n",
    "        diff = self_measure.shape[0] - conv_measure.shape[0]\n",
    "        self_measure = self_measure[:-diff]\n",
    "    return self_measure, conv_measure\n",
    "\n",
    "\n",
    "def get_F0s_for_plots(wav_nr, vocoder, target_speakers, t_f0s):\n",
    "\n",
    "    colours = []\n",
    "    labels = []\n",
    "    f0s = []\n",
    "    vocoder_generated = self_convert(wav_nr, vocoder=vocoder)\n",
    "    self_f0 = get_F0(vocoder_generated)\n",
    "\n",
    "    first_flag = True\n",
    "\n",
    "    for (key, f0) in zip(target_speakers, t_f0s):\n",
    "        self_f0, f0 = get_trimmed(self_f0, f0)\n",
    "\n",
    "        if first_flag:\n",
    "            times = librosa.times_like(f0)\n",
    "            f0s.append(self_f0)\n",
    "            colours.append('black')\n",
    "            labels.append('Self')\n",
    "            first_flag = False\n",
    "\n",
    "        f0s.append(f0)\n",
    "        colours.append('lightcoral' if gender_map[int(\n",
    "            key.replace('p', ''))] == F else 'green')\n",
    "        labels.append(pseudo_names[int(key)])\n",
    "\n",
    "    return f0s, colours, labels, times\n",
    "\n",
    "\n",
    "def get_rmse_for_plots(wav_nr, vocoder, target_speakers, converted_samples):\n",
    "    '''\n",
    "\n",
    "    Args:\n",
    "        wav_nr:\n",
    "        vocoder:\n",
    "        target_speakers:\n",
    "        converted_samples:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    '''\n",
    "    intnsts = []\n",
    "    colours = []\n",
    "    labels = []\n",
    "    vocoder_generated = self_convert(wav_nr, vocoder=vocoder)\n",
    "    self_intnsty = get_intensity(vocoder_generated)\n",
    "    first_flag = False\n",
    "    for key in target_speakers:\n",
    "        audio_ = converted_samples[key]\n",
    "\n",
    "        intnsty = get_intensity(audio_)\n",
    "        self_intnsty, intnsty = get_trimmed(self_intnsty, intnsty)\n",
    "\n",
    "        if not first_flag:\n",
    "            times = librosa.times_like(intnsty)\n",
    "            intnsts.append(self_intnsty)\n",
    "            colours.append('black')\n",
    "            labels.append('Self')\n",
    "            first_flag = True\n",
    "\n",
    "        intnsts.append(intnsty)\n",
    "        colours.append('lightcoral' if gender_map[int(\n",
    "            key.replace('p', ''))] == F else 'green')\n",
    "        labels.append(pseudo_names[int(key)])\n",
    "    return intnsts, colours, labels, times\n",
    "\n",
    "\n",
    "def get_F0s(wav_nr, vocoder, target_speakers, converted_samples):\n",
    "    f0s = []\n",
    "    colours = []\n",
    "    labels = []\n",
    "    vocoder_generated = self_convert(wav_nr, vocoder=vocoder)\n",
    "    self_f0 = get_F0(vocoder_generated)\n",
    "    first_flag = True\n",
    "    for key in target_speakers:\n",
    "        audio_ = converted_samples[key]\n",
    "\n",
    "        f0 = get_F0(audio_)\n",
    "        self_f0, f0 = get_trimmed(self_f0, f0)\n",
    "\n",
    "        if first_flag:\n",
    "            times = librosa.times_like(f0)\n",
    "\n",
    "            f0s.append(self_f0)\n",
    "            colours.append('black')\n",
    "            labels.append('self')\n",
    "            first_flag = False\n",
    "\n",
    "        f0s.append(f0)\n",
    "        colours.append('lightcoral' if gender_map[int(\n",
    "            key.replace('p', ''))] == F else 'green')\n",
    "        labels.append(pseudo_names[int(key)])\n",
    "    return f0s, colours, labels, times\n",
    "\n",
    "\n",
    "def get_recording(recorder):\n",
    "\n",
    "    with open('recording.wav', 'wb') as f:\n",
    "        f.write(recorder.audio.value)\n",
    "    #!ffmpeg -i recording.wav -ac 1 -f wav file.wav -y -hide_banner -loglevel panic\n",
    "    audio_np, orig_sr = librosa.load('recording.wav')\n",
    "    audio = audio_np / np.max(np.abs(audio_np))\n",
    "    wav = librosa.resample(audio, orig_sr=orig_sr, target_sr=24000)\n",
    "\n",
    "    # wav_nr = nr.reduce_noise(y=wav, sr=24000, n_std_thresh_stationary=2,stationary=True, prop_decrease=0.9)\n",
    "    wav_nr = nr.reduce_noise(\n",
    "        y=wav, sr=24000, thresh_n_mult_nonstationary=1.5, stationary=False, prop_decrease=0.7)\n",
    "\n",
    "    return wav/np.max(np.abs(wav)), wav_nr/np.max(np.abs(wav_nr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068c2c0",
   "metadata": {},
   "source": [
    "## Record !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = CameraStream(constraints={\"audio\": True, \"video\": False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdec100",
   "metadata": {},
   "source": [
    "## Let's Hear !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, wav_nr = get_recording(recorder)\n",
    "print(\"The noise-filtered recording...\")\n",
    "display(Audio(wav_nr, rate=sr, autoplay=False))\n",
    "print(\"The original recording...\")\n",
    "display(Audio(wav, rate=sr, autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a90f4",
   "metadata": {},
   "source": [
    "## It looks like ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wave\n",
    "time = np.round(wav_nr.shape[0] / sr)\n",
    "plt.plot(wav, alpha=0.5, color=\"lightcoral\")\n",
    "plt.plot(wav_nr, alpha=0.5, color=\"cyan\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Original and Noise Filtered Recording\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot mel-spectrogram along with F0\n",
    "f0 = get_F0(wav)\n",
    "f0_nr = get_F0(wav_nr)\n",
    "D_orig = get_melspec(wav)\n",
    "D_filt = get_melspec(wav_nr)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "librosa.display.specshow(D_orig, x_axis=\"time\", y_axis=\"log\", ax=ax1)\n",
    "librosa.display.specshow(D_filt, x_axis=\"time\", y_axis=\"log\", ax=ax2)\n",
    "\n",
    "times = librosa.times_like(f0)\n",
    "\n",
    "ax1.plot(times, f0, label=\"f0\", color=\"cyan\", linewidth=2)\n",
    "ax1.title.set_text(\"Original\")\n",
    "ax2.plot(times, f0_nr, label=\"Pitch\", color=\"cyan\", linewidth=2)\n",
    "ax2.title.set_text(\"Noise Filtered\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6474af6",
   "metadata": {},
   "source": [
    "## Let's Listen to the Conversions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43dac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emostargan, F0_model, vocoder, reference_embeddings = build_inference_pipeline(\n",
    "    device='cpu')\n",
    "\n",
    "print('Using noise-filtered recording:\\n------------------------------------')\n",
    "\n",
    "target_speakers, converted_samples = generate_conversions(wav_nr,\n",
    "\n",
    "                                                          emostargan,\n",
    "\n",
    "                                                          reference_embeddings,\n",
    "\n",
    "                                                          F0_model,\n",
    "\n",
    "                                                          vocoder,\n",
    "\n",
    "                                                          noise_reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a9933-418e-4ca9-8869-092c9c35a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using original recording:\\n------------------------------------\")\n",
    "target_speakers, converted_samples = generate_conversions(\n",
    "    wav, emostargan, reference_embeddings, F0_model, vocoder, noise_reduce=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9438327",
   "metadata": {},
   "source": [
    "## More Insights...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a0faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0s = plot_melspec(target_speakers, converted_samples, noise_reduce=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f1d60",
   "metadata": {},
   "source": [
    "## Change in Pitch (Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f4efee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust this to remove noise at the beginning of recording\n",
    "strt_idx = 50\n",
    "# adjust this to remove noise at the end of recording\n",
    "end_idx = -1\n",
    "plt.figure(figsize=(15, 5))\n",
    "tf0s, colours, labels, ftimes = get_F0s_for_plots(\n",
    "    wav_nr, vocoder, target_speakers, f0s)\n",
    "for idx in range(len(f0s)):\n",
    "    plt.plot(ftimes[strt_idx:end_idx], tf0s[idx][strt_idx:end_idx],\n",
    "             color=colours[idx], linestyle='--' if idx == 0 else '-')\n",
    "\n",
    "get_gender_legend()\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Pitch (Hz)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c09af9",
   "metadata": {},
   "source": [
    "## Change in Pitch (Speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76697de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for idx in range(len(f0s)):\n",
    "    plt.plot(ftimes[strt_idx:], tf0s[idx][strt_idx:],\n",
    "             label='Self' if idx == 0 else labels[idx],\n",
    "             color='black' if idx == 0 else list(\n",
    "                 mcolors.TABLEAU_COLORS.values())[idx],\n",
    "             linestyle='--' if idx == 0 else '-')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Pitch (Hz)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dafaa7c",
   "metadata": {},
   "source": [
    "## Change in Intensity (Gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ec155",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "intensities, colours, labels, itimes = get_rmse_for_plots(\n",
    "    wav_nr, vocoder, target_speakers, converted_samples)\n",
    "for idx in range(len(f0s)):\n",
    "    plt.plot(itimes[strt_idx:], intensities[idx][strt_idx:],\n",
    "             color=colours[idx], linestyle='--' if idx == 0 else '-')\n",
    "\n",
    "get_gender_legend()\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Intensity')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c1892",
   "metadata": {},
   "source": [
    "## Change in Intensity (Speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b69cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "for idx in range(len(intensities)):\n",
    "    plt.plot(itimes[strt_idx:], intensities[idx][strt_idx:],\n",
    "             label='Self' if idx == 0 else labels[idx],\n",
    "             color='black' if idx == 0 else list(\n",
    "                 mcolors.TABLEAU_COLORS.values())[idx],\n",
    "             linestyle='--' if idx == 0 else '-')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Time (sec)')\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

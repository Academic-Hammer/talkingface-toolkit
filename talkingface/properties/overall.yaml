# Enviroment Settings
gpu_id: '3, 4, 5'                     # (str) The id of GPU device(s).
worker: 0                       # (int) The number of workers processing the data.
use_gpu: True                   # (bool) Whether or not to use GPU.
seed: 2023                      # (int) Random seed.
checkpoint_dir: 'saved'         # (str) The path to save checkpoint file.
show_progress: True             # (bool) Whether or not to show the progress bar of every epoch. 
log_wandb: False                # (bool) Whether or not to use Weights & Biases(W&B).
shuffle: True                   # (bool) Whether or not to shuffle the training data before each epoch.
device: 'cuda'
reproducibility: True           # (bool) Whether or not to make results reproducible.

# Training Settings
epochs: 1                     # (int) The number of training epochs.
train_batch_size: 2048          # (int) The training batch size.
learner: adam                   # (str) The name of used optimizer.
learning_rate: 0.0001            # (float) Learning rate.
eval_step: 1                    # (int) The number of training epochs before an evaluation on the valid dataset.
stopping_step: 10               # (int) The threshold for validation-based early stopping.
weight_decay: 0.0               # (float) The weight decay value (L2 penalty) for optimizers.
saved: True
resume: False
train: True

# Evaluation Settings
metrics: ["LSE", "SSIM"]
evaluate_batch_size: 50          # (int) The evaluation batch size.
lse_checkpoint_path: 'checkpoints/LSE/syncnet_v2.model'
temp_dir: 'results/temp'
lse_reference_dir: 'lse'
valid_metric_bigger: False
# (bool) Whether to take a bigger valid metric value as a better result.



# Syncnet
syncnet_wt: 0.03 # (int) is initially zero, will be set automatically to 0.03 later.Leads to faster convergence.
syncnet_batch_size: 64 # (int) batch_size for syncnet train
syncnet_lr: 0.0001 #(float) learning rate for syncnet train
syncnet_eval_interval: 10000
syncnet_checkpoint_interval: 10000
syncnet_T: 5
syncnet_mel_step_size: 16
syncnet_checkpoint_path: "checkpoints/wav2lip/lipsync_expert.pth"

# Data preprocessing for Wav2lip
num_mels: 80
rescale: True
rescaling_max: 0.9
use_lws: False
n_fft: 800
hop_size: 200
win_size: 800
sample_rate: 16000
frame_shift_ms: None
signal_normalization: True
allow_clipping_in_normalization: True
symmetric_mels: True
max_abs_value: 4
preemphasize: True
preemphasis: 0.97
min_level_db: -100
ref_level_db: 20
fmin: 55
fmax: 7600
img_size: 96
fps: 25
mel_step_size: 16

batch_size: 16
ngpu: 1


# Train
checkpoint_sub_dir: "/wav2lip" # 和overall.yaml里checkpoint_dir拼起来作为最终目录

temp_sub_dir: "/wav2lip" # 和overall.yaml里temp_dir拼起来作为最终目录


# Inference
pads: [0, 10, 0, 0]
static: False
face_det_batch_size: 16
resize_factor: 1
crop: [0, -1, 0, -1]
box: [-1, -1, -1, -1]
rotate: False
nosmooth: False
wav2lip_batch_size: 128
vshift: 15

train_filelist: 'train' # 当前数据集的数据划分文件 train
test_filelist: 'test' # 当前数据集的数据划分文件 test
val_filelist: 'val' # 当前数据集的数据划分文件 val

data_root: 'dataset/lrs2/data/main' # 当前数据集的数据根目录
preprocessed_root: 'dataset/lrs2/preprocessed_data' # 当前数据集的预处理数据根目录

need_preprocess: True # 数据集是否需要预处理，如抽帧、抽音频等

preprocess_batch_size: 32
